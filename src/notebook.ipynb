{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16, VGG16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')  # Navigate up to GitHub folder\n",
    "from LeNet_5.classes.convolution import ConvolutionLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def reshape_output(out_tensor):\n",
    "    batch_size, features = out_tensor.shape\n",
    "    height = width = int(sqrt(features//512))\n",
    "    return out_tensor.reshape(\n",
    "        batch_size, \n",
    "        512, \n",
    "        height,\n",
    "        width,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(512, 4096, 7)\n",
    "lin = nn.Linear(in_features=100, out_features=100)\n",
    "lin.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipConnection(nn.Module):\n",
    "    '''\n",
    "    applies a convolution, upsamples 2x, and stores the resulting output,\n",
    "    but passes the raw output forward to the next layer\n",
    "\n",
    "    '''\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        # pool4 has 512 output channels\n",
    "        # 30 output channels for each class\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=num_classes, kernel_size=1, stride=1)\n",
    "        # upsample layer\n",
    "        self.val = None\n",
    "        self.last_in = None\n",
    "        # initialize weights with 0\n",
    "        nn.init.zeros_(self.conv.weight) \n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        returns the input as is but stores the result of a convolution.\n",
    "        \n",
    "        args\n",
    "        - x: tensor(batch_size, 512, img_height/16, img_width/16)\n",
    "        \n",
    "        '''\n",
    "        # apply convolution, upsample 2x, and store\n",
    "        self.val = self.conv(x)\n",
    "        # apply nothing to x\n",
    "        self.last_in = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(img):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 224])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_img = Image.open('../data/custom/cat108.jpg')\n",
    "lion_img = Image.open('../data/custom/lion.jpg')\n",
    "\n",
    "cat_tensor = to_tensor(cat_img)[:3,:,:].unsqueeze(0)\n",
    "lion_tensor = to_tensor(lion_img)[:3,:,:].unsqueeze(0)\n",
    "batch = torch.cat([cat_tensor, cat_tensor], dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): SkipConnection(\n",
       "    (conv): Conv2d(128, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (11): SkipConnection(\n",
       "    (conv): Conv2d(128, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): SkipConnection(\n",
       "    (conv): Conv2d(256, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (20): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (21): ReLU(inplace=True)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (27): SkipConnection(\n",
       "    (conv): Conv2d(512, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (35): SkipConnection(\n",
       "    (conv): Conv2d(512, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# get the vgg-16 pretrained model used in paper\n",
    "model = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "\n",
    "# get weights and biases from last 3 fully connected layers in VGG-16\n",
    "linear1_weights = model.classifier[0].weight.data\n",
    "linear1_bias = model.classifier[0].bias.data\n",
    "linear2_weights = model.classifier[3].weight.data\n",
    "linear2_bias = model.classifier[3].bias.data\n",
    "\n",
    "# reshape to fit convolution layer dimensions\n",
    "reshaped_weights1 = linear1_weights.reshape(4096, 512, 7, 7)\n",
    "reshaped_weights2 = linear2_weights.reshape(4096, 4096, 1, 1)\n",
    "\n",
    "# use pytorch's convolution layer class\n",
    "conv1 = nn.Conv2d(in_channels=512, out_channels=4096, kernel_size=7)\n",
    "conv2 = nn.Conv2d(in_channels=4096, out_channels=4096, kernel_size=1)\n",
    "conv3 = nn.Conv2d(in_channels=4096, out_channels=30, kernel_size=1)\n",
    "\n",
    "# replace default weights with trained reshaped weights\n",
    "conv1.weight.data = reshaped_weights1\n",
    "conv2.weight.data = reshaped_weights2\n",
    "\n",
    "# replace biases\n",
    "conv1.bias.data = linear1_bias\n",
    "conv2.bias.data = linear2_bias\n",
    "\n",
    "# replace linear layers with convlayers\n",
    "model.classifier[0] = conv1\n",
    "model.classifier[3] = conv2\n",
    "model.classifier[6] = conv3\n",
    "\n",
    "# remove average pooling layer\n",
    "model.avgpool = nn.Identity()\n",
    "\n",
    "# initialize post-pool 1x1 conv predictions\n",
    "pool3_conv = SkipConnection(in_channels=256, num_classes=30)\n",
    "pool4_conv = SkipConnection(in_channels=512, num_classes=30)\n",
    "size_check_1 = SkipConnection(in_channels=128, num_classes=30)\n",
    "size_check_2 = SkipConnection(in_channels=128, num_classes=30)\n",
    "size_check_3 = SkipConnection(in_channels=512, num_classes=30)\n",
    "\n",
    "# insert conv_wrap in sequential after pool3 and pool4\n",
    "model.features.insert(17, pool3_conv)\n",
    "model.features.insert(25, pool4_conv)\n",
    "model.features.insert(6, size_check_1)\n",
    "model.features.insert(11, size_check_2)\n",
    "model.features.insert(35, size_check_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.features(batch)\n",
    "pred = model.classifier(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 30, 28, 28])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "upsample = nn.ConvTranspose2d(in_channels=30, out_channels=30, kernel_size=4,stride=2, bias=False)\n",
    "# pred2 = nn.functional.interpolate(pred, (1024, 1024), mode='bilinear')\n",
    "pred2 = upsample(pred)\n",
    "pred2.shape\n",
    "pred.shape\n",
    "# pred2 = upsample3(pred2)\n",
    "model.features[19].val.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solving equations below to determine stride and kernel size for 2x upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output = (input - 1) * stride + kernel\n",
    "output = 2*input\n",
    "\n",
    "2*i = (i - 1) * s + k\n",
    "\n",
    "(k-2)/(2-s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
